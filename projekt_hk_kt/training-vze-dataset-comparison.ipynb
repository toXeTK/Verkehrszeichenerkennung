{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6bb1bd-f7ad-42b9-a5ba-1e1b72e36c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Import\n",
    "# =============================================================================\n",
    "#import sys\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, utils, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To get reproducable results with the same training setting random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Flags\n",
    "plots_on = True\n",
    "test_on_specific_image = True\n",
    "data_augmentation = False\n",
    "set_early_stopping = False\n",
    "patience_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define functions\n",
    "# =============================================================================\n",
    "# Change image size and convert to grayscale images\n",
    "def pic_prep (image, x, y):\n",
    "  image = cv2.resize(image, (y,x)) # change image size\n",
    "  #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "  image = image.astype('float32') / 255.0 # image normalization\n",
    "  return image\n",
    "\n",
    "# Shuffle images\n",
    "def unison_shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "# Save training info in file.txt\n",
    "def write_infotext(filepath,\n",
    "                   image_size,\n",
    "                   num_neurons_per_layer,\n",
    "                   batch_size,\n",
    "                   epochs,\n",
    "                   history=[0,1],\n",
    "                   testresult=['tr']):\n",
    "    with open(filepath + 'info.txt','w') as txt:\n",
    "        txt.write('############ SPECS ############\\n'\n",
    "                  '\\n'\n",
    "                  'IMG Resolution: ' f'{image_size}\\n'\n",
    "                  'Neurons per Layer: ' f'{num_neurons_per_layer}\\n'\n",
    "                  'Batch Size: ' f'{batch_size}\\n'\n",
    "                  'Epochs: ' f'{epochs}\\n'\n",
    "                  '\\n'\n",
    "                  '\\n'\n",
    "                  '########### HISTORY ###########\\n'\n",
    "                  '\\n'\n",
    "                  'Test score: ' f'{history[0]}\\n'\n",
    "                  'Test accuracy: ' f'{history[1]}\\n'\n",
    "                  '\\n'\n",
    "                  '\\n'\n",
    "                  '########### TESTING ###########\\n'\n",
    "                  '\\n'\n",
    "                  'Test1: Achtung, Test2: Fuenfzig, Test3: Hundert, Test4: Stop, Test5: Vorfahrt, Test6: VorfahrtGewaehren\\n'\n",
    "                  '\\n')\n",
    "        \n",
    "        for i in range(0,len(testresult),1):\n",
    "            txt.write(f'{testresult[i]}\\n')\n",
    "\n",
    "        txt.write('\\n'\n",
    "                  '###############################\\n')\n",
    "\n",
    "class callback_print(callbacks.Callback):\n",
    "    SHOW_NUMBER = 10\n",
    "    counter = 0\n",
    "    epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if self.counter == self.SHOW_NUMBER or self.epoch == 1:\n",
    "            print('Epoch: ' + str(self.epoch) + ' loss: ' + str(logs['loss']))\n",
    "            if self.epoch > 1:\n",
    "                self.counter = 0\n",
    "        self.counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0226f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function with some hypterparameters to change for comparison\n",
    "# =============================================================================\n",
    "def train_signs_model(num_training=0,\n",
    "                      image_size=[32, 28, 1],\n",
    "                      num_neurons_per_layer=[64,32,16,13],\n",
    "                      batch_size=4,\n",
    "                      epochs=300):\n",
    "    # =============================================================================\n",
    "    # Declare variables\n",
    "    # =============================================================================\n",
    "    img_size_x = image_size[0]\n",
    "    img_size_y = image_size[1]\n",
    "    \n",
    "\n",
    "    # Change the following path to your dataset path\n",
    "    path = '/home/pi/Documents'\n",
    "    #path = '/home/metzgeka/ml_project/datasets'\n",
    "    dataset_dir = path + '/Verkehrszeichenerkennung/projekt_hk_kt/dataset/'\n",
    "    checkpoint_filepath = path + '/Verkehrszeichenerkennung/projekt_hk_kt/chpt/' + f'{num_training}/'\n",
    "    os.makedirs(checkpoint_filepath, exist_ok=True)\n",
    "    img_path = glob.glob(dataset_dir + '*/*.png')\n",
    "    anz_data = len(img_path)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Get number of data (image/label) \n",
    "    # =============================================================================\n",
    "    anz_data = int(anz_data) - 1\n",
    "    dataset = np.zeros((anz_data, img_size_x, img_size_y, 3), dtype=float)\n",
    "    ground_truth = np.zeros((anz_data), dtype=float)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Callbacks for training\n",
    "    # =============================================================================\n",
    "    model_filepath=checkpoint_filepath + str(num_training) + '-chpt.model.keras'\n",
    "\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        model_filepath,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_accuracy',\n",
    "        mode='auto',\n",
    "        save_best_only=True,\n",
    "        verbose = 1)\n",
    "    \n",
    "    #early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=patience_epochs,          #modify for range in x loss improvement\n",
    "                                   restore_best_weights=True)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Read dataset\n",
    "    # =============================================================================\n",
    "    for i in range(0, anz_data, 1):\n",
    "        image = cv2.imread(img_path[i])                         # read image\n",
    "        image = pic_prep(image, img_size_x, img_size_y)         # prepare image\n",
    "        dataset[i,:,:,:] = image                                # 2d-image to 3d-array\n",
    "        gt_path = os.path.normpath(img_path[i]).split(os.sep)   # splitting paths for folder numbers\n",
    "        ground_truth[i] = int(gt_path[-2])                      # ground truth for all images\n",
    "\n",
    "    # =============================================================================\n",
    "    # Prepare dataset: train set (80%) and test set (20%)\n",
    "    # =============================================================================\n",
    "    #dataset = dataset.reshape(anz_data, img_dim) # convert into 2d array (all pixel in one row)\n",
    "    ground_truth = ground_truth.reshape(anz_data, 1)\n",
    "\n",
    "    dataset, ground_truth = unison_shuffle(dataset, ground_truth)\n",
    "\n",
    "    trainset = np.random.choice(dataset.shape[0],\n",
    "                                int(dataset.shape[0]*0.80), \n",
    "                                replace=False)\n",
    "    train_data = dataset[trainset,:]\n",
    "    train_gt = ground_truth[trainset]\n",
    "    train_gt = utils.to_categorical(train_gt, 6)  #geändert auf 6 klassen von 13\n",
    "\n",
    "    testset = np.delete(np.arange(0, len(ground_truth) ), \n",
    "                        trainset) \n",
    "    test_data = dataset[testset,:]\n",
    "    test_gt = ground_truth[testset]\n",
    "    test_gt = utils.to_categorical(test_gt, 6)  #auch geändert\n",
    "    \n",
    "    if(data_augmentation):\n",
    "        #define ImageDataGenerator for data augmentation\n",
    "        train_datagen = ImageDataGenerator(rotation_range=30,\n",
    "                                           width_shift_range=0.2,\n",
    "                                           height_shift_range=0.2,\n",
    "                                           shear_range=0.2,\n",
    "                                           zoom_range=0.2,\n",
    "                                           horizontal_flip=True,\n",
    "                                           fill_mode='nearest')\n",
    "        \n",
    "        #create validation generator without augmentation\n",
    "        val_datagen = ImageDataGenerator()\n",
    "\n",
    "        #apply data augmentation to the training set\n",
    "        train_generator = train_datagen.flow(train_data, train_gt, batch_size=32)\n",
    "        val_generator = val_datagen.flow(test_data, test_gt, batch_size=32)\n",
    "\n",
    "    # =============================================================================\n",
    "    # Create neural network\n",
    "    # =============================================================================\n",
    "    img_shape = (image_size[0],image_size[1],3)\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(6, (5,5), activation=None, padding='same', input_shape=img_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(16, (5,5), activation=None))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(120, (5,5), activation=None))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(num_neurons_per_layer[0], activation=None))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(num_neurons_per_layer[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # =============================================================================\n",
    "    # Train the neuronal network\n",
    "    # =============================================================================\n",
    "    if set_early_stopping:\n",
    "        callback=[early_stopping]\n",
    "    else:\n",
    "        callback=[model_checkpoint_callback]\n",
    "    \n",
    "    if data_augmentation:\n",
    "        history = model.fit(train_generator,\n",
    "                            steps_per_epoch=len(train_data)//32,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=val_generator,\n",
    "                            validation_steps=len(test_data)//32,\n",
    "                            callbacks=callback)\n",
    "    else:\n",
    "        history = model.fit(train_data,\n",
    "                            train_gt,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=0,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(test_data, test_gt),\n",
    "                            callbacks=callback)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Plot train and val accuracy.\n",
    "    # =============================================================================\n",
    "    if plots_on:\n",
    "        plt.plot(history.history['accuracy'], label='accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(checkpoint_filepath + 'acc.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # =========================================================================\n",
    "        # Plot train and val loss.\n",
    "        # =========================================================================\n",
    "        plt.plot(history.history['loss'], label='loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.savefig(checkpoint_filepath + 'loss.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        #generate and plot confusion matrix\n",
    "        y_pred_probs = model.predict(test_data)\n",
    "        y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "        y_true_classes = np.argmax(test_gt, axis=1)\n",
    "        cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                      display_labels=['Achtung','Fuenfzig','Hundert','Stop','Vorfahrt','VorfahrtGewaehren'])\n",
    "        disp.plot(cmap='Blues', xticks_rotation=45)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(checkpoint_filepath + 'conf_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    if test_on_specific_image:\n",
    "        # =========================================================================\n",
    "        # Load pretrained dataset weights to e.g. test on new (unseen) data.\n",
    "        # =========================================================================\n",
    "        model.load_weights(model_filepath)   \n",
    "\n",
    "        # =========================================================================\n",
    "        # Test dataset on xxx.\n",
    "        # =========================================================================\n",
    "        score = model.evaluate(test_data, test_gt, verbose=1)\n",
    "        print('Test score:', score[0])\n",
    "        print('Test accuracy:', score[1])       \n",
    "\n",
    "        # =========================================================================\n",
    "        # Testing on a single image.\n",
    "        # =========================================================================\n",
    "\n",
    "        result = ['tr0','tr1','tr2','tr3','tr4','tr5']\n",
    "        \n",
    "        # Class Labels\n",
    "        class_lables = ['Achtung',\n",
    "                        #'Achtzig',\n",
    "                        #'Baustelle',\n",
    "                        #'Dreisig',\n",
    "                        #'DurchfahrtVerboten',\n",
    "                        'Fuenfzig',                        # geändert auf 6 klassen\n",
    "                        'Hundert',\n",
    "                        #'Hundertdreisig',\n",
    "                        #'Sechzig',\n",
    "                        #'Siebzig',\n",
    "                        'Stop',\n",
    "                        'Vorfahrt',\n",
    "                        'VofahrtGewaehren']\n",
    "        for i in range(0,6,1):\n",
    "            data_pred = np.zeros((1, img_size_x, img_size_y,3), dtype=float)\n",
    "            img_pred = cv2.imread('/home/pi/Documents/Verkehrszeichenerkennung/projekt_hk_kt/testdata/' + f'{i}')\n",
    "            img_pred = pic_prep(img_pred, img_size_x, img_size_y)\n",
    "            data_pred[0,:,:,:] = img_pred\n",
    "            #data_pred = data_pred.reshape(1,img_dim)\n",
    "            #print(np.shape(data_pred))\n",
    "            probs = model.predict(data_pred)\n",
    "            result[i] = [{label: f'{prob * 100:.2f}%' \n",
    "                        for label, prob in zip(class_lables, sample)}\n",
    "                        for sample in probs]\n",
    "        \n",
    "            print('Percentages per class:\\n')\n",
    "            print(f'{result[i]}\\n')\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Output class: \n",
    "        # =============================================================================\n",
    "        \"\"\"\n",
    "        max_res = 0\n",
    "        res_index = 12\n",
    "        for i in range(0, 12, 1):\n",
    "            if probs[0,i] > max_res:\n",
    "                max_res = probs[0,i]\n",
    "                res_index = i\n",
    "\n",
    "        if res_index == 0:\n",
    "            print('Schild: 'f'{class_lables[0]} erkannt')\n",
    "        elif res_index == 1:\n",
    "            print('Schild: 'f'{class_lables[1]} erkannt')\n",
    "        elif res_index == 2:\n",
    "            print('Schild: 'f'{class_lables[2]} erkannt')\n",
    "        elif res_index == 3:\n",
    "            print('Schild: 'f'{class_lables[3]} erkannt')\n",
    "        elif res_index == 4:\n",
    "            print('Schild: 'f'{class_lables[4]} erkannt')\n",
    "        elif res_index == 5:\n",
    "            print('Schild: 'f'{class_lables[5]} erkannt')\n",
    "        elif res_index == 6:\n",
    "            print('Schild: 'f'{class_lables[6]} erkannt')\n",
    "        elif res_index == 7:\n",
    "            print('Schild: 'f'{class_lables[7]} erkannt')\n",
    "        elif res_index == 8:\n",
    "            print('Schild: 'f'{class_lables[8]} erkannt')\n",
    "        elif res_index == 9:\n",
    "            print('Schild: 'f'{class_lables[9]} erkannt')\n",
    "        elif res_index == 10:\n",
    "            print('Schild: 'f'{class_lables[10]} erkannt')\n",
    "        elif res_index == 11:\n",
    "            print('Schild: 'f'{class_lables[11]} erkannt')\n",
    "        elif res_index == 12:\n",
    "            print('Schild: 'f'{class_lables[12]} erkannt')\n",
    "        elif res_index == 13:\n",
    "            print('Error!')\n",
    "        \"\"\"\n",
    "    write_infotext(checkpoint_filepath,\n",
    "                   image_size,\n",
    "                   num_neurons_per_layer,\n",
    "                   batch_size,\n",
    "                   epochs,\n",
    "                   score,\n",
    "                   result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4eafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train multiple networks with differend image scales and neurons per layer\n",
    "# =============================================================================\n",
    "\n",
    "image_dims = [[64, 56],\n",
    "              [64, 56],\n",
    "              [64, 56],\n",
    "              [64, 56],\n",
    "              [64, 56]]\n",
    "\n",
    "neurons_layers = [[128,6],\n",
    "                  [102,6],\n",
    "                  [84,6],\n",
    "                  [64,6],\n",
    "                  [42,6]]\n",
    "\n",
    "\n",
    "# Train multiple times with different hyper parameters (dim img size, neurons per layer)\n",
    "\n",
    "# Edit custom training parameters:\n",
    "training_count = 42\n",
    "batch_count = 16\n",
    "epoch_count = 60\n",
    "\n",
    "for img_size, num_neurons in zip(image_dims, neurons_layers):\n",
    "    print(\"Training\", training_count, \"with image size\", img_size, \"and\", num_neurons, \"neurons per layer\")\n",
    "    train_signs_model(training_count, img_size, num_neurons, batch_count, epoch_count)\n",
    "    print(\"#################################################\\n\")\n",
    "    training_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e4766",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
